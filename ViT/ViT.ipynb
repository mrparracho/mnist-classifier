{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very primitive example e2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 7 128 16\n",
      "torch.Size([1, 4, 4, 7, 7]) - patches\n",
      "torch.Size([1, 16, 49]) - patches_flat\n",
      "torch.Size([1, 16, 128]) - embedded_patches\n",
      "torch.Size([1, 16, 128]) - embedded_patches_with_pos\n",
      "torch.Size([1, 17, 128]) - embedded_patches_with_class\n",
      "tensor([[[ 0.7384, -0.0144,  0.6405,  ...,  0.4617,  0.3223, -1.2893],\n",
      "         [ 0.3264,  0.1468, -0.8783,  ..., -0.2934, -2.0359,  1.0929],\n",
      "         [-2.9512, -1.8823,  0.4007,  ..., -1.7324, -0.0732,  1.3046],\n",
      "         ...,\n",
      "         [ 0.0313, -0.0473,  0.4422,  ...,  0.4195, -0.5974, -1.4699],\n",
      "         [-0.6585,  0.4810, -0.2090,  ...,  0.5938, -0.3574, -0.7519],\n",
      "         [ 1.6199, -2.7024, -2.0224,  ...,  0.6610, -0.7109, -0.8421]]],\n",
      "       grad_fn=<CatBackward0>) - H\n",
      "torch.Size([128, 128]) torch.Size([128, 128]) torch.Size([128, 128]) torch.Size([128, 128])\n",
      "torch.Size([128, 128]) torch.Size([128, 128]) torch.Size([128, 128]) torch.Size([128, 128])\n",
      "torch.Size([128, 128]) torch.Size([128, 128]) torch.Size([128, 128]) torch.Size([128, 128])\n",
      "tensor([[[ 0.0354, -0.1090,  0.0835,  ...,  0.0956, -0.0351, -0.0457],\n",
      "         [ 0.0354, -0.1090,  0.0835,  ...,  0.0956, -0.0351, -0.0457],\n",
      "         [ 0.0354, -0.1090,  0.0835,  ...,  0.0956, -0.0351, -0.0457],\n",
      "         ...,\n",
      "         [ 0.0354, -0.1090,  0.0835,  ...,  0.0956, -0.0351, -0.0457],\n",
      "         [ 0.0354, -0.1090,  0.0835,  ...,  0.0956, -0.0351, -0.0457],\n",
      "         [ 0.0354, -0.1090,  0.0835,  ...,  0.0956, -0.0351, -0.0457]]],\n",
      "       grad_fn=<ViewBackward0>) - H\n",
      "Class token shape: torch.Size([1, 128])\n",
      "Logits shape: torch.Size([1, 10])\n",
      "Probability distribution shape: torch.Size([1, 10])\n",
      "Probability distribution: tensor([[0.1105, 0.0947, 0.1102, 0.0885, 0.0957, 0.1029, 0.0996, 0.1022, 0.1034,\n",
      "         0.0922]], grad_fn=<SoftmaxBackward0>)\n",
      "Predicted class: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from os import path\n",
    "\n",
    "# Image dimensions\n",
    "pixel_size = 28 # 28x28 image\n",
    "patch_size = 7  # 7x7 patches (28/7 = 4 patches per side)\n",
    "num_patches = (pixel_size // patch_size) ** 2  # 4*4 = 16 patches\n",
    "embed_dim = 128\n",
    "print(pixel_size, patch_size, embed_dim, num_patches)\n",
    "\n",
    "def preprocess_data():\n",
    "\n",
    "    # Randomly generate image tensor\n",
    "    image = torch.randn(1, pixel_size, pixel_size)  # (batch_size, 28, 28)\n",
    "\n",
    "    # 1. Patch embedding\n",
    "    # Reshape image to patches using unfold\n",
    "    # unfold(dim, size, step) - creates patches of size x size with step stride\n",
    "    patches = image.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
    "    print(f'{patches.shape} - patches') # (batch_size, 7, 7, 4, 4)\n",
    "\n",
    "    # Reshape to (batch_size, num_patches, patch_size*patch_size)\n",
    "    patches_flat = patches.reshape(1, num_patches, patch_size * patch_size)\n",
    "    print(f'{patches_flat.shape} - patches_flat') # (batch_size, 16, 49)\n",
    "\n",
    "    # 2. Linear Projection to Embed Patches\n",
    "    patch_embedding = nn.Linear(patch_size * patch_size, embed_dim)\n",
    "    embedded_patches = patch_embedding(patches_flat)\n",
    "    print(f'{embedded_patches.shape} - embedded_patches') # (batch, 16, 128)\n",
    "\n",
    "    # 3. Positional Embedding\n",
    "    # Add positional embeddings\n",
    "    position_embeddings = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "    embedded_patches_with_pos = embedded_patches + position_embeddings\n",
    "    print(f'{embedded_patches_with_pos.shape} - embedded_patches_with_pos') # (batch, 16, 128)\n",
    "\n",
    "    # 4. Class Token Addition\n",
    "    class_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "    embedded_patches_with_class = torch.cat([class_token, embedded_patches_with_pos], dim=1)\n",
    "    print(f'{embedded_patches_with_class.shape} - embedded_patches_with_class') # (batch, 17, 128)\n",
    "\n",
    "    return embedded_patches_with_class\n",
    "\n",
    "# Encoder\n",
    "def encoder(embedded_patches_with_class,embed_dim):\n",
    "    W_q = nn.Linear(embed_dim, embed_dim)\n",
    "    W_k = nn.Linear(embed_dim, embed_dim)\n",
    "    W_v = nn.Linear(embed_dim, embed_dim)\n",
    "    W_o = nn.Linear(embed_dim, embed_dim)\n",
    "    print(W_q.weight.shape, W_k.weight.shape, W_v.weight.shape, W_o.weight.shape)\n",
    "\n",
    "    K = embedded_patches_with_class @ W_k.weight.T # (batch_size, 17, 128)\n",
    "    Q = embedded_patches_with_class @ W_q.weight.T # (batch_size, 17, 128)\n",
    "    V = embedded_patches_with_class @ W_v.weight.T # (batch_size, 17, 128)\n",
    "    A = Q @ K.transpose(-1, -2) # (batch_size, 17, 17)\n",
    "    A = A / (embed_dim ** 0.5) # scale by sqrt(d_k) to avoid large values\n",
    "    A = torch.softmax(A, dim=-1)  # Convert to probabilities\n",
    "    H = A @ V # (batch_size, 17, 128) # Raw attention output\n",
    "    H  = W_o(H) # (batch_size, 17, 128) # Output of the attention layer\n",
    "\n",
    "    # print(f'{K.shape} - K')\n",
    "    # print(f'{Q.shape} - Q')\n",
    "    # print(f'{V.shape} - V')\n",
    "    # print(f'{A.shape} - A')\n",
    "    # print(f'{H.shape} - H')\n",
    "    \n",
    "    return H\n",
    "\n",
    "def classification_head(H):\n",
    "    # Linear classifier\n",
    "    Linear_classifier = nn.Linear(embed_dim, 10)\n",
    "\n",
    "    # Extract only the class token (first token)\n",
    "    class_token_output = H[:, 0, :]  # Shape: (1, 128)\n",
    "    print(f\"Class token shape: {class_token_output.shape}\")\n",
    "\n",
    "    # Apply classifier only to class token\n",
    "    logits = Linear_classifier(class_token_output)  # Shape: (1, 10)\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "    # Get prediction\n",
    "    probability_dist = torch.softmax(logits, dim=1)\n",
    "    print(f\"Probability distribution shape: {probability_dist.shape}\")\n",
    "    print(f\"Probability distribution: {probability_dist}\")\n",
    "    predicted_class = torch.argmax(probability_dist, dim=1)\n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "\n",
    "embedded_patches_with_class = preprocess_data()\n",
    "layers = 3\n",
    "H = embedded_patches_with_class\n",
    "print(f'{H} - H')\n",
    "for i in range(layers):\n",
    "    H = encoder(H,embed_dim)\n",
    "\n",
    "print(f'{H} - H')\n",
    "\n",
    "classification_head(H)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring the code into PyTorch Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17, 128]) - patch_embedded_image1\n",
      "torch.Size([1, 17, 128]) - patch_embedded_image1\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size # 7\n",
    "        self.embed_dim = embed_dim # 128\n",
    "        self.num_patches = (image_size // patch_size) ** 2 # 16\n",
    "        \n",
    "        # Define layers and parameters\n",
    "        self.patch_embedding = nn.Linear(patch_size * patch_size, embed_dim) # 49 -> 128\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim)) # 17, 128\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim)) # 1, 1, 128\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # x can be (1, 1, 28, 28) or (1, 1, 40, 40) where batch_size is dynamic (batch_size, channels, image_w, image_h)\n",
    "        # 1. Patch tokenization\n",
    "        x = x.squeeze(1) # (1, 1, 28, 28) -> (1, 28, 28) drops the channel dimension\n",
    "        patches = x.unfold(1, self.patch_size, self.patch_size).unfold(2, self.patch_size, self.patch_size) # (1, 7, 7, 4, 4)\n",
    "\n",
    "        patches_flat = patches.reshape(x.shape[0], self.num_patches, self.patch_size * self.patch_size) # (1, 16, 49)\n",
    "        # print(f'{patches_flat.shape} - patches_flat')\n",
    "        \n",
    "        # 2. Linear projection (use class attribute)\n",
    "        embedded_patches = self.patch_embedding(patches_flat) # (1, 1, 16, 128)\n",
    "        \n",
    "        # 3. Add positional embeddings (use class attribute)\n",
    "        embedded_patches_with_pos = embedded_patches + self.position_embedding[:, 1:, :]  # Skip class token position # (1, 1, 17, 128)\n",
    "        \n",
    "        # 4. Add class token (use class attribute)\n",
    "        embedded_patches_with_class = torch.cat([self.class_token.expand(x.shape[0], -1, -1), embedded_patches_with_pos], dim=1) # (1, 1, 17, 128)\n",
    "        \n",
    "        return embedded_patches_with_class\n",
    "\n",
    "\n",
    "image_size = 28\n",
    "patch_size = 7\n",
    "embed_dim = 128\n",
    "batch_size = 1\n",
    "\n",
    "# Create the module once\n",
    "patch_embedding = PatchEmbedding(image_size=image_size, patch_size=patch_size, embed_dim=embed_dim)\n",
    "\n",
    "# Use it multiple times with the same learned weights\n",
    "image1 = torch.randn(batch_size, image_size, image_size)\n",
    "image2 = torch.randn(batch_size, image_size, image_size)\n",
    "\n",
    "\n",
    "patch_embedded_image1 = patch_embedding(image1)  # Uses learned weights\n",
    "patch_embedded_image1 = patch_embedding(image2)  # Uses same learned weights\n",
    "\n",
    "print(f'{patch_embedded_image1.shape} - patch_embedded_image1')\n",
    "print(f'{patch_embedded_image1.shape} - patch_embedded_image1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0043,  0.0032, -0.0449,  ..., -0.0895,  0.0277, -0.0102],\n",
      "         [ 0.0043,  0.0032, -0.0449,  ..., -0.0895,  0.0277, -0.0102],\n",
      "         [ 0.0043,  0.0032, -0.0449,  ..., -0.0895,  0.0277, -0.0102],\n",
      "         ...,\n",
      "         [ 0.0043,  0.0032, -0.0449,  ..., -0.0895,  0.0277, -0.0102],\n",
      "         [ 0.0043,  0.0032, -0.0449,  ..., -0.0895,  0.0277, -0.0102],\n",
      "         [ 0.0043,  0.0032, -0.0449,  ..., -0.0895,  0.0277, -0.0102]]],\n",
      "       grad_fn=<AddBackward0>) - H\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Define the linear layers (same as your function)\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim) \n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "        self.mlp_up = nn.Linear(embed_dim, 4*embed_dim) # optional\n",
    "        self.mlp_down = nn.Linear(4*embed_dim, embed_dim) # optional\n",
    "\n",
    "    def forward(self, embedded_patches_with_class):\n",
    "        # Print weight shapes (same as your function)\n",
    "        # print(self.W_q.weight.shape, self.W_k.weight.shape, self.W_v.weight.shape, self.W_o.weight.shape)\n",
    "\n",
    "        # Compute Q, K, V (exactly the same logic)\n",
    "        K = embedded_patches_with_class @ self.W_k.weight.T  # (batch_size, 17, 128)\n",
    "        Q = embedded_patches_with_class @ self.W_q.weight.T  # (batch_size, 17, 128)\n",
    "        V = embedded_patches_with_class @ self.W_v.weight.T  # (batch_size, 17, 128)\n",
    "        \n",
    "        # Compute attention scores (exactly the same logic)\n",
    "        A = Q @ K.transpose(-1, -2)  # (batch_size, 17, 17)\n",
    "        A = A / (self.embed_dim ** 0.5)  # scale by sqrt(d_k) to avoid large values\n",
    "        A = torch.softmax(A, dim=-1)  # Convert to probabilities\n",
    "        \n",
    "        # Apply attention (exactly the same logic)\n",
    "        H = A @ V  # (batch_size, 17, 128) # Raw attention output\n",
    "        H = self.W_o(H)  # (batch_size, 17, 128) # Output of the attention layer\n",
    "\n",
    "        # MLP - optional\n",
    "        H_residual = self.mlp_up(H)\n",
    "        H_residual = torch.relu(H_residual)\n",
    "        H = H + self.mlp_down(H_residual)\n",
    "\n",
    "        # Uncomment these if you want the same debug prints\n",
    "        # print(f'{K.shape} - K')\n",
    "        # print(f'{Q.shape} - Q')\n",
    "        # print(f'{V.shape} - V')\n",
    "        # print(f'{A.shape} - A')\n",
    "        # print(f'{H.shape} - H')\n",
    "        \n",
    "        return H\n",
    "    \n",
    "\n",
    "# Create encoder once\n",
    "encoder = Encoder(embed_dim=128)\n",
    "\n",
    "layers = 3\n",
    "# Use it multiple times (same weights)\n",
    "H = patch_embedded_image1\n",
    "for i in range(layers):\n",
    "    H = encoder(H)  # Uses the same learned weights each time\n",
    "\n",
    "print(f'{H} - H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9]) - predicted_class\n"
     ]
    }
   ],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, embed_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define the linear classifier (same as your function)\n",
    "        self.Linear_classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, H):\n",
    "        # Extract only the class token (first token) - same logic\n",
    "        class_token_output = H[:, 0, :]  # Shape: (batch_size, 128)\n",
    "        # print(f\"Class token shape: {class_token_output.shape}\")\n",
    "\n",
    "        # Apply classifier only to class token - same logic\n",
    "        logits = self.Linear_classifier(class_token_output)  # Shape: (batch_size, 10)\n",
    "        # print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "        # Get prediction - same logic\n",
    "        probability_dist = torch.softmax(logits, dim=1)\n",
    "        # print(f\"Probability distribution shape: {probability_dist.shape}\")\n",
    "        # print(f\"Probability distribution: {probability_dist}\")\n",
    "        # predicted_class = torch.argmax(probability_dist, dim=1)\n",
    "        # print(f\"Predicted class: {predicted_class}\")\n",
    "        \n",
    "        return probability_dist\n",
    "\n",
    "# Create classification head\n",
    "classification_head = ClassificationHead(embed_dim=128, num_classes=10)\n",
    "\n",
    "# Use it (same as your function)\n",
    "predicted_class = torch.argmax(classification_head(H), dim=1)  # Uses the same learned weights\n",
    "print(f'{predicted_class} - predicted_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0946, 0.1023, 0.1049, 0.0923, 0.0909, 0.0905, 0.1124, 0.1001, 0.1096,\n",
      "         0.1024],\n",
      "        [0.0945, 0.1022, 0.1044, 0.0919, 0.0910, 0.0906, 0.1126, 0.1001, 0.1101,\n",
      "         0.1025],\n",
      "        [0.0945, 0.1023, 0.1045, 0.0923, 0.0908, 0.0907, 0.1123, 0.1002, 0.1097,\n",
      "         0.1028],\n",
      "        [0.0948, 0.1019, 0.1047, 0.0921, 0.0910, 0.0909, 0.1120, 0.1001, 0.1100,\n",
      "         0.1026],\n",
      "        [0.0942, 0.1021, 0.1044, 0.0920, 0.0905, 0.0908, 0.1122, 0.1004, 0.1107,\n",
      "         0.1027]], grad_fn=<SoftmaxBackward0>) - predictions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_dim, num_layers, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.patch_embedding = PatchEmbedding(image_size=image_size, patch_size=patch_size, embed_dim=embed_dim)\n",
    "        \n",
    "        # Create multiple encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            Encoder(embed_dim=embed_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.classification_head = ClassificationHead(embed_dim=embed_dim, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        \n",
    "        # Pass through each encoder layer\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "        \n",
    "        x = self.classification_head(x)\n",
    "        return x\n",
    "\n",
    "image_size = 28\n",
    "patch_size = 7\n",
    "embed_dim = 128\n",
    "batch_size = 5\n",
    "num_encoder_layers = 3\n",
    "num_classes = 10\n",
    "\n",
    "# Create the complete model with 3 layers\n",
    "model = Transformer(\n",
    "    image_size=image_size, \n",
    "    patch_size=patch_size, \n",
    "    embed_dim=embed_dim, \n",
    "    num_layers=num_encoder_layers, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Single forward pass handles all layers\n",
    "images = torch.randn(batch_size, image_size, image_size)  # batch of 4 images\n",
    "predictions = model(images)  # Automatically goes through all 3 layers\n",
    "print(f'{predictions} - predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x12fa14e80> - train_loader\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x128353d00> - test_loader\n"
     ]
    }
   ],
   "source": [
    "# training a ViT model\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. Load the data\n",
    "# 2. Define the model\n",
    "# 3. Define the loss function\n",
    "# 4. Define the optimizer\n",
    "# 5. Train the model\n",
    "# 6. Evaluate the model\n",
    "\n",
    "# 1. Load the data\n",
    "\n",
    "def visualize_image(dataset):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Get a sample image\n",
    "    image, label = dataset[0]\n",
    "\n",
    "    # Convert to numpy and remove channel dimension\n",
    "    image_np = image.squeeze().numpy()  # Shape: (28, 28)\n",
    "\n",
    "    print(f\"Image shape: {image_np.shape}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Pixel values (first 5x5):\\n{image_np[:5, :5]}\")\n",
    "\n",
    "    # Visualize the image\n",
    "    plt.imshow(image_np, cmap='gray')\n",
    "    plt.title(f'Digit: {label}')\n",
    "    plt.show()\n",
    "\n",
    "def get_data_loaders(batch_size=64, data_dir=None):\n",
    "    \"\"\"\n",
    "    Create data loaders for training and testing MNIST dataset.\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): Batch size for training and testing\n",
    "        data_dir (str): Directory to store the dataset\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    if data_dir is None:\n",
    "        data_dir = os.path.join(\"./data\")\n",
    "    \n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "    ])\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Download and load the training data\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=data_dir, \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Download and load the test data\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root=data_dir, \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_data_loaders(batch_size=64, data_dir=None)\n",
    "print(f'{train_loader} - train_loader')\n",
    "print(f'{test_loader} - test_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the model\n",
    "image_size = 28\n",
    "patch_size = 7\n",
    "embed_dim = 128\n",
    "batch_size = 5\n",
    "num_encoder_layers = 3\n",
    "num_classes = 10\n",
    "\n",
    "# Create the complete model with 3 layers\n",
    "model = Transformer(\n",
    "    image_size=image_size, \n",
    "    patch_size=patch_size, \n",
    "    embed_dim=embed_dim, \n",
    "    num_layers=num_encoder_layers, \n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the loss function and optimizer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 938/938 [00:18<00:00, 50.37it/s, loss=2.32]\n",
      "Epoch 1/10 [Test]: 100%|██████████| 157/157 [00:02<00:00, 54.21it/s, accuracy=12.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Test Accuracy: 12.37%\n",
      "New best model saved with accuracy: 12.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 938/938 [00:18<00:00, 50.47it/s, loss=2.34]\n",
      "Epoch 2/10 [Test]: 100%|██████████| 157/157 [00:02<00:00, 53.04it/s, accuracy=12.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Test Accuracy: 12.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 938/938 [00:18<00:00, 50.38it/s, loss=2.34]\n",
      "Epoch 3/10 [Test]: 100%|██████████| 157/157 [00:02<00:00, 53.80it/s, accuracy=12.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Test Accuracy: 12.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 938/938 [00:19<00:00, 48.07it/s, loss=2.34]\n",
      "Epoch 4/10 [Test]: 100%|██████████| 157/157 [00:03<00:00, 44.82it/s, accuracy=12.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Test Accuracy: 12.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 938/938 [00:19<00:00, 48.94it/s, loss=2.34]\n",
      "Epoch 5/10 [Test]: 100%|██████████| 157/157 [00:02<00:00, 53.59it/s, accuracy=12.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Test Accuracy: 12.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 938/938 [00:19<00:00, 48.74it/s, loss=2.34]\n",
      "Epoch 6/10 [Test]: 100%|██████████| 157/157 [00:03<00:00, 50.79it/s, accuracy=12.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Test Accuracy: 12.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 938/938 [00:19<00:00, 49.15it/s, loss=2.34]\n",
      "Epoch 7/10 [Test]: 100%|██████████| 157/157 [00:03<00:00, 50.42it/s, accuracy=12.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Test Accuracy: 12.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 938/938 [00:18<00:00, 49.80it/s, loss=2.34]\n",
      "Epoch 8/10 [Test]: 100%|██████████| 157/157 [00:02<00:00, 52.88it/s, accuracy=12.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Test Accuracy: 12.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 938/938 [00:19<00:00, 49.24it/s, loss=2.34]\n",
      "Epoch 9/10 [Test]: 100%|██████████| 157/157 [00:03<00:00, 51.85it/s, accuracy=12.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Test Accuracy: 12.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 938/938 [00:19<00:00, 48.95it/s, loss=2.34]\n",
      "Epoch 10/10 [Test]: 100%|██████████| 157/157 [00:02<00:00, 52.65it/s, accuracy=12.4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Test Accuracy: 12.37%\n",
      "Training completed. Best accuracy: 12.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (patch_embedding): PatchEmbedding(\n",
       "    (patch_embedding): Linear(in_features=49, out_features=128, bias=True)\n",
       "  )\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-2): 3 x Encoder(\n",
       "      (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mlp_up): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (mlp_down): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): ClassificationHead(\n",
       "    (Linear_classifier): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Train the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def train_model(model, train_loader, test_loader, device, epochs=10, learning_rate=0.001, \n",
    "                checkpoint_dir=\"./checkpoints\"):\n",
    "    \"\"\"\n",
    "    Train the MNIST model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model\n",
    "        train_loader (DataLoader): DataLoader for training data\n",
    "        test_loader (DataLoader): DataLoader for test data\n",
    "        device (torch.device): Device to train on (CPU or GPU)\n",
    "        epochs (int): Number of training epochs\n",
    "        learning_rate (float): Learning rate for the optimizer\n",
    "        checkpoint_dir (str): Directory to save model checkpoints\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "    \"\"\"\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "            # Move data to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"loss\": running_loss / (batch_idx + 1)})\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{epochs} [Test]\")\n",
    "            for data, target in progress_bar:\n",
    "                # Move data to device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                \n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                \n",
    "                # Update counters\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                accuracy = 100 * correct / total\n",
    "                progress_bar.set_postfix({\"accuracy\": accuracy})\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Test Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Save model if it's the best so far\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, \"mnist_model.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'accuracy': accuracy,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"New best model saved with accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Save checkpoint for every epoch\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'accuracy': accuracy,\n",
    "        }, checkpoint_path)\n",
    "    \n",
    "    print(f\"Training completed. Best accuracy: {best_accuracy:.2f}%\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Check for CUDA availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "train_model(model, train_loader, test_loader, device, epochs=10, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluate the model\n",
    "\n",
    "# 6. Save the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
