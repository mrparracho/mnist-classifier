#!/usr/bin/env python3
"""
Benchmark Multi-Head Latent Attention (MLA) vs Standard Attention.
Tests memory usage, inference speed, and maintains accuracy verification.
"""

import torch
import torch.nn as nn
import time
import sys
import os
from typing import Dict, List, Tuple, Optional
import gc
import tracemalloc
from pathlib import Path

# Add paths for imports
sys.path.append('/Users/admin/git/mnist-classifier')
sys.path.append('/Users/admin/git/mnist-classifier/models')
os.chdir('/Users/admin/git/mnist-classifier')

from models.encoder_decoder.model import EncoderDecoder
from models.encoder_decoder.mla_attention import calculate_mla_compression


class StandardAttentionModel(EncoderDecoder):
    """Version with standard attention for comparison"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # Replace MLA attention layers with standard attention
        # This would require implementing standard attention classes
        # For now, we'll compare against the existing model
        pass


def get_model_memory_usage(model: nn.Module) -> int:
    """Calculate model memory usage in bytes"""
    param_size = 0
    buffer_size = 0
    
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    
    return param_size + buffer_size


def benchmark_inference_speed(
    model: nn.Module,
    batch_size: int = 4,
    num_samples: int = 1000,
    max_length: int = 20,
    device: str = 'cpu'
) -> Dict[str, float]:
    """Benchmark inference speed for autoregressive generation"""
    
    model.to(device)
    model.eval()
    
    # Generate dummy data
    images = torch.randn(batch_size, 1, 280, 280, device=device)
    
    # Warmup runs
    print("  Warming up...")
    for _ in range(10):
        with torch.no_grad():
            _ = model(images, max_length=max_length)
    
    # Synchronize for accurate timing
    if device == 'cuda':
        torch.cuda.synchronize()
    
    # Actual benchmark
    print(f"  Running {num_samples // batch_size} batches...")
    start_time = time.time()
    total_tokens = 0
    
    with torch.no_grad():
        for i in range(num_samples // batch_size):
            output = model(images, max_length=max_length)
            total_tokens += output.shape[0] * output.shape[1]
            
            # Progress indicator
            if (i + 1) % 50 == 0:
                print(f"    Completed {i + 1} batches...")
    
    # Synchronize and calculate timing
    if device == 'cuda':
        torch.cuda.synchronize()
    
    end_time = time.time()
    total_time = end_time - start_time
    
    samples_per_sec = num_samples / total_time
    tokens_per_sec = total_tokens / total_time
    
    return {
        'total_time': total_time,
        'samples_per_sec': samples_per_sec,
        'tokens_per_sec': tokens_per_sec,
        'total_tokens': total_tokens,
        'total_samples': num_samples
    }


def benchmark_memory_usage(
    model: nn.Module,
    batch_size: int = 4,
    max_length: int = 20,
    device: str = 'cpu'
) -> Dict[str, float]:
    """Benchmark memory usage during inference"""
    
    model.to(device)
    model.eval()
    
    # Clear cache and start memory tracing
    gc.collect()
    if device == 'cuda':
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
    
    tracemalloc.start()
    
    # Generate dummy data
    images = torch.randn(batch_size, 1, 280, 280, device=device)
    
    # Measure memory during inference
    with torch.no_grad():
        output = model(images, max_length=max_length)
    
    # Get memory statistics
    current_mem, peak_mem = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    results = {
        'model_memory_mb': get_model_memory_usage(model) / 1024 / 1024,
        'peak_memory_mb': peak_mem / 1024 / 1024,
        'current_memory_mb': current_mem / 1024 / 1024,
    }
    
    if device == 'cuda':
        results.update({
            'cuda_memory_mb': torch.cuda.memory_allocated() / 1024 / 1024,
            'cuda_peak_memory_mb': torch.cuda.max_memory_allocated() / 1024 / 1024,
        })
    
    return results


def test_accuracy_preservation(
    model: nn.Module,
    batch_size: int = 8,
    num_tests: int = 100,
    device: str = 'cpu'
) -> Dict[str, float]:
    """Test that MLA preserves accuracy compared to standard attention"""
    
    model.to(device)
    model.eval()
    
    # Generate test data
    images = torch.randn(batch_size, 1, 280, 280, device=device)
    target_sequences = torch.randint(0, 13, (batch_size, 10), device=device)
    
    total_loss = 0.0
    valid_outputs = 0
    
    criterion = nn.CrossEntropyLoss(ignore_index=12)  # Ignore padding token
    
    with torch.no_grad():
        for i in range(num_tests // batch_size):
            # Forward pass
            output = model(images, target_sequence=target_sequences)
            
            # Calculate loss
            loss = criterion(
                output.view(-1, output.size(-1)),
                target_sequences.view(-1)
            )
            
            if not torch.isnan(loss):
                total_loss += loss.item()
                valid_outputs += 1
    
    avg_loss = total_loss / valid_outputs if valid_outputs > 0 else float('inf')
    
    return {
        'average_loss': avg_loss,
        'valid_outputs': valid_outputs,
        'total_tests': num_tests
    }


def compare_compression_ratios(
    embed_dim: int = 128,
    num_heads: int = 16,
    num_layers: int = 16,
    sequence_lengths: List[int] = [10, 20, 50, 100]
) -> Dict[str, any]:
    """Compare theoretical memory compression ratios"""
    
    results = {}
    
    for seq_len in sequence_lengths:
        compression_stats = calculate_mla_compression(
            embed_dim=embed_dim,
            num_heads=num_heads,
            num_layers=num_layers,
            kv_lora_rank=embed_dim // 3,  # 3x compression (our setting)
            seq_len=seq_len
        )
        
        results[f'seq_len_{seq_len}'] = compression_stats
    
    return results


def run_comprehensive_benchmark(
    embed_dim: int = 128,
    num_heads: int = 16,
    num_layers: int = 16,
    batch_size: int = 4,
    num_samples: int = 1000,
    device: str = 'cpu'
) -> Dict[str, any]:
    """Run comprehensive benchmark comparing MLA vs theoretical standard attention"""
    
    print("üöÄ Starting Multi-Head Latent Attention Benchmark")
    print("=" * 60)
    
    # Create MLA model (our current implementation)
    print("üìä Creating MLA model...")
    mla_model = EncoderDecoder(
        image_size=280,
        patch_size=14,
        encoder_embed_dim=embed_dim,
        decoder_embed_dim=embed_dim,
        num_layers=num_layers,
        num_heads=num_heads,
        dropout=0.1,
        max_seq_len=102
    )
    
    print(f"  ‚úÖ Model created with {sum(p.numel() for p in mla_model.parameters()):,} parameters")
    
    # Benchmark results container
    results = {
        'model_config': {
            'embed_dim': embed_dim,
            'num_heads': num_heads,
            'num_layers': num_layers,
            'batch_size': batch_size,
            'num_samples': num_samples,
            'device': device
        }
    }
    
    # 1. Memory Analysis
    print("\nüìà Memory Analysis")
    print("-" * 30)
    
    memory_results = benchmark_memory_usage(mla_model, batch_size, device=device)
    print(f"  Model memory: {memory_results['model_memory_mb']:.1f} MB")
    print(f"  Peak inference memory: {memory_results['peak_memory_mb']:.1f} MB")
    
    results['memory'] = memory_results
    
    # 2. Theoretical Compression Analysis
    print("\nüîç Theoretical Compression Analysis")
    print("-" * 40)
    
    compression_results = compare_compression_ratios(embed_dim, num_heads, num_layers)
    
    for seq_len, stats in compression_results.items():
        print(f"  {seq_len}:")
        print(f"    Compression ratio: {stats['compression_ratio']:.2f}x")
        print(f"    Memory saved: {stats['memory_saved_percent']:.1f}%")
    
    results['compression'] = compression_results
    
    # 3. Speed Benchmarks for Different Sequence Lengths
    print("\n‚ö° Speed Benchmarks")
    print("-" * 25)
    
    speed_results = {}
    test_lengths = [10, 20, 30, 50]
    
    for max_length in test_lengths:
        print(f"\n  Testing sequence length {max_length}:")
        speed_result = benchmark_inference_speed(
            mla_model, batch_size, num_samples, max_length, device
        )
        
        print(f"    Samples/sec: {speed_result['samples_per_sec']:.1f}")
        print(f"    Tokens/sec: {speed_result['tokens_per_sec']:.1f}")
        print(f"    Total time: {speed_result['total_time']:.2f}s")
        
        speed_results[f'length_{max_length}'] = speed_result
    
    results['speed'] = speed_results
    
    # 4. Accuracy Test
    print("\nüéØ Accuracy Preservation Test")
    print("-" * 35)
    
    accuracy_results = test_accuracy_preservation(mla_model, batch_size, device=device)
    print(f"  Average loss: {accuracy_results['average_loss']:.4f}")
    print(f"  Valid outputs: {accuracy_results['valid_outputs']}/{accuracy_results['total_tests']}")
    
    results['accuracy'] = accuracy_results
    
    # 5. Parameter Efficiency Analysis
    print("\nüìã Parameter Efficiency Analysis")
    print("-" * 40)
    
    total_params = sum(p.numel() for p in mla_model.parameters())
    attention_params = 0
    
    # Count attention-related parameters
    for name, module in mla_model.named_modules():
        if 'attention' in name.lower() or 'mla' in name.lower():
            attention_params += sum(p.numel() for p in module.parameters())
    
    param_efficiency = {
        'total_parameters': total_params,
        'attention_parameters': attention_params,
        'attention_percentage': (attention_params / total_params) * 100,
        'non_attention_parameters': total_params - attention_params
    }
    
    print(f"  Total parameters: {param_efficiency['total_parameters']:,}")
    print(f"  Attention parameters: {param_efficiency['attention_parameters']:,}")
    print(f"  Attention percentage: {param_efficiency['attention_percentage']:.1f}%")
    
    results['parameters'] = param_efficiency
    
    return results


def print_summary_report(results: Dict[str, any]):
    """Print a comprehensive summary report"""
    
    print("\n" + "=" * 60)
    print("üìä MLA BENCHMARK SUMMARY REPORT")
    print("=" * 60)
    
    config = results['model_config']
    print(f"Model: {config['embed_dim']}d, {config['num_heads']} heads, {config['num_layers']} layers")
    print(f"Benchmark: {config['num_samples']} samples, batch size {config['batch_size']}")
    
    # Memory Summary
    print(f"\nüß† MEMORY EFFICIENCY:")
    memory = results['memory']
    print(f"  Model size: {memory['model_memory_mb']:.1f} MB")
    print(f"  Peak inference: {memory['peak_memory_mb']:.1f} MB")
    
    # Compression Summary
    print(f"\nüóúÔ∏è THEORETICAL COMPRESSION (vs Standard Attention):")
    compression = results['compression']
    seq_20_stats = compression.get('seq_len_20', {})
    seq_50_stats = compression.get('seq_len_50', {})
    
    if seq_20_stats:
        print(f"  20 tokens: {seq_20_stats['compression_ratio']:.1f}x smaller, {seq_20_stats['memory_saved_percent']:.0f}% saved")
    if seq_50_stats:
        print(f"  50 tokens: {seq_50_stats['compression_ratio']:.1f}x smaller, {seq_50_stats['memory_saved_percent']:.0f}% saved")
    
    # Speed Summary
    print(f"\n‚ö° INFERENCE SPEED:")
    speed = results['speed']
    for length_key, speed_data in speed.items():
        length = length_key.split('_')[1]
        print(f"  {length} tokens: {speed_data['samples_per_sec']:.1f} samples/sec")
    
    # Accuracy Summary
    print(f"\nüéØ MODEL ACCURACY:")
    accuracy = results['accuracy']
    print(f"  Average loss: {accuracy['average_loss']:.4f}")
    print(f"  Success rate: {(accuracy['valid_outputs']/accuracy['total_tests'])*100:.1f}%")
    
    # Parameter Efficiency
    print(f"\nüìà PARAMETER EFFICIENCY:")
    params = results['parameters']
    print(f"  Total parameters: {params['total_parameters']:,}")
    print(f"  Attention efficiency: {params['attention_percentage']:.1f}% of model")
    
    print("\n" + "=" * 60)
    print("‚úÖ Benchmark completed successfully!")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Benchmark MLA performance")
    parser.add_argument("--batch-size", type=int, default=4, help="Batch size for benchmarking")
    parser.add_argument("--num-samples", type=int, default=1000, help="Number of samples to test")
    parser.add_argument("--device", type=str, default="cpu", choices=["cpu", "cuda", "mps"], help="Device to run on")
    parser.add_argument("--embed-dim", type=int, default=128, help="Model embedding dimension")
    parser.add_argument("--num-heads", type=int, default=16, help="Number of attention heads")
    parser.add_argument("--num-layers", type=int, default=16, help="Number of transformer layers")
    
    args = parser.parse_args()
    
    # Set device
    if args.device == "cuda" and torch.cuda.is_available():
        device = "cuda"
    elif args.device == "mps" and torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
    
    print(f"Using device: {device}")
    
    # Run benchmark
    results = run_comprehensive_benchmark(
        embed_dim=args.embed_dim,
        num_heads=args.num_heads,
        num_layers=args.num_layers,
        batch_size=args.batch_size,
        num_samples=args.num_samples,
        device=device
    )
    
    # Print summary
    print_summary_report(results)
    
    # Save results
    import json
    output_file = f"mla_benchmark_results_{int(time.time())}.json"
    with open(output_file, 'w') as f:
        # Convert any non-serializable objects to strings
        serializable_results = {}
        for key, value in results.items():
            if isinstance(value, dict):
                serializable_results[key] = {k: float(v) if isinstance(v, (int, float)) else str(v) for k, v in value.items()}
            else:
                serializable_results[key] = str(value)
        
        json.dump(serializable_results, f, indent=2)
    
    print(f"\nüíæ Results saved to: {output_file}") 